{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Code to launch complete cloud architecture code and tear it down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda ARN:  arn:aws:lambda:us-east-1:589049386593:function:a5_lambda\n",
      "Bucket name:  mariagabrielaa-a5\n",
      "Number of current tables:  0\n",
      "Table creation time:  2023-04-20 17:27:13.653000-05:00\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "###########################################################################\n",
    "##############          (A) LAUNCH CLOUD ARCHITECTURE         #############\n",
    "###########################################################################\n",
    "\n",
    "#---------------------(1) CREATE LAMBDA FUNCTION--------------------------\n",
    "\n",
    "# Access IAM role to allow Lambda to interact with other AWS resources\n",
    "aws_lambda = boto3.client('lambda')\n",
    "iam_client = boto3.client('iam')\n",
    "role = iam_client.get_role(RoleName='LabRole')\n",
    "\n",
    "# Open zipped directory that contains lambda function\n",
    "with open('a5.zip', 'rb') as f:\n",
    "    lambda_zip = f.read() \n",
    "\n",
    "try:\n",
    "    # If function hasn't yet been created, create it\n",
    "    response = aws_lambda.create_function(\n",
    "        FunctionName='a5_lambda',\n",
    "        Runtime='python3.9',\n",
    "        Role=role['Role']['Arn'], \n",
    "        Handler='lambda_function.lambda_handler', \n",
    "        Code=dict(ZipFile=lambda_zip),\n",
    "        Timeout=300\n",
    "    )\n",
    "except aws_lambda.exceptions.ResourceConflictException:\n",
    "    # If function already exists, update it based on zip file contents\n",
    "    response = aws_lambda.update_function_code(\n",
    "        FunctionName='a5_lambda',\n",
    "        ZipFile=lambda_zip\n",
    "        )\n",
    "\n",
    "lambda_arn = response['FunctionArn']\n",
    "print(\"Lambda ARN: \", lambda_arn)\n",
    "\n",
    "#---------------------(2) CREATE S3 BUCKET-------------------------------\n",
    "\n",
    "# Initialize s3 client and resource\n",
    "s3 = boto3.client('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "# Create S3 bucket\n",
    "s3.create_bucket(Bucket='mariagabrielaa-a5')\n",
    "\n",
    "# Get the list_buckets response\n",
    "response = s3.list_buckets()\n",
    "\n",
    "# Print each bucket name (to check bucket was created)\n",
    "for bucket in response['Buckets']:\n",
    "    print(\"Bucket name: \", bucket['Name'])\n",
    "\n",
    "\n",
    "#---------------------(3) CREATE DYNAMO DB TABLE-------------------------\n",
    "\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "\n",
    "table = dynamodb.create_table(\n",
    "    TableName='a5_results',\n",
    "    KeySchema=[\n",
    "        {\n",
    "            'AttributeName': 'user_id',\n",
    "            'KeyType': 'HASH'\n",
    "        }\n",
    "    ],\n",
    "    AttributeDefinitions=[\n",
    "        {\n",
    "            'AttributeName': 'user_id',\n",
    "            'AttributeType': 'S'\n",
    "        }\n",
    "    ],\n",
    "    ProvisionedThroughput={\n",
    "        'ReadCapacityUnits': 1,\n",
    "        'WriteCapacityUnits': 1\n",
    "    }\n",
    ")\n",
    "\n",
    "# Wait until AWS confirms that table exists before moving on\n",
    "table.meta.client.get_waiter('table_exists').wait(TableName='a5_results')\n",
    "\n",
    "# get data about table (should currently be no items in table)\n",
    "print(\"Number of current tables: \", table.item_count)\n",
    "print(\"Table creation time: \", table.creation_date_time)\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "##############         (B) TEAR DOWN CLOUD ARCHITECTURE       #############\n",
    "###########################################################################\n",
    "\n",
    "#----------------------(1) DELETE S3 OBJECTS------------------------------\n",
    "\n",
    "def remove_objects(bucket_name):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "    for item in bucket.objects.all():\n",
    "        item.delete()\n",
    "\n",
    "#---------------------(2) DELETE DYNAMO DB TABLE--------------------------\n",
    "\n",
    "def delete_table(table_object):\n",
    "    table_object.delete()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Modify Lambda function code to store raw JSON data (i.e. the input payload) for valid survey submissions as a JSON file in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The updated lambda function is in 'lambda_q2.py' and for reference, it looks like this:\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "s3 = boto3.resource('s3') \n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    key = event[\"user_id\"] + event[\"timestamp\"] + \".json\"\n",
    "\n",
    "    if event[\"time_elapsed\"] <= 3:\n",
    "        return {\n",
    "        'statusCode': 400,\n",
    "        'body': 'Invalid entry. Not enough time elapsed.'\n",
    "        }\n",
    "    elif event[\"freetext\"] == \"\":\n",
    "        return {\n",
    "        'statusCode': 400,\n",
    "        'body':'Invalid entry. No text response'\n",
    "        }\n",
    "    else:\n",
    "        # Only upload valid survey entries to S3 bucket\n",
    "        s3.Bucket('mariagabrielaa-a5').put_object(Key=key, Body=json.dumps(event))\n",
    "\n",
    "        return {\n",
    "        'statusCode': 200,\n",
    "        'body': 'OK'\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "For each valid survey submission, modify your Lambda function to insert/update the participant’s record into a DynamoDB database, with the User’s ID number as the partition key, along with their answers to each of the survey questions (including the written response), as well as the number of times the user has completed a survey overall (including the current survey you are processing).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The updated lambda function is in 'lambda_q3.py' and for reference, it looks like this:\n",
    "import boto3\n",
    "import json\n",
    "from boto3.dynamodb.conditions import Key\n",
    "\n",
    "s3 = boto3.resource('s3') \n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    key = event[\"user_id\"] + event[\"timestamp\"] + \".json\"\n",
    "\n",
    "    if event[\"time_elapsed\"] <= 3:\n",
    "        return {\n",
    "        'statusCode': 400,\n",
    "        'body': 'Invalid entry. Not enough time elapsed.'\n",
    "        }\n",
    "    elif event[\"freetext\"] == \"\":\n",
    "        return {\n",
    "        'statusCode': 400,\n",
    "        'body':'Invalid entry. No text response'\n",
    "        }\n",
    "    else:\n",
    "        # Only upload valid survey entries to S3 bucket\n",
    "        s3.Bucket('mariagabrielaa-a5').put_object(Key=key, Body=json.dumps(event))\n",
    "\n",
    "        table = dynamodb.Table(\"a5_results\")\n",
    "        \n",
    "        # First, check if there is already an entry in the table for the user_id\n",
    "        response = table.get_item(\n",
    "            Key={'user_id': event[\"user_id\"]}\n",
    "            )\n",
    "        # If no previous entry, then set num_submission to 0\n",
    "        if \"Item\" not in response:\n",
    "            num_submission = 0\n",
    "        # Otherwise, query the previous number of submissions\n",
    "        else:\n",
    "            response = table.query(KeyConditionExpression=Key('user_id').eq(event[\"user_id\"]),\n",
    "            ProjectionExpression='num_submission')\n",
    "            num_submission = response['Items'][0]['num_submission']\n",
    "\n",
    "        # Insert survey entry in DynamoDb table\n",
    "        table.put_item(Item= {\n",
    "                            'user_id': event['user_id'],\n",
    "                            'q1': event['q1'],\n",
    "                            'q2': event['q2'],\n",
    "                            'q3': event['q3'],\n",
    "                            'q4': event['q4'],\n",
    "                            'q5': event['q5'],\n",
    "                            'freetext': event['freetext'],\n",
    "                            'num_submission': int(num_submission) + 1\n",
    "                            }\n",
    "                        )\n",
    "        return {\n",
    "        'statusCode': 200,\n",
    "        'body': 'OK. DynamoDB table updated and response uploaded to S3'\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Check whether your Lambda function is working properly. Specifically, you should invoke your function with the following test data input from participants 1, 2, 3, 4, and 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'statusCode': 200,\n",
       " 'body': 'OK. DynamoDB table updated and response uploaded to S3'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "p1_response_1 = {\n",
    "    \"user_id\": \"0001\",\n",
    "    \"timestamp\": \"092821120000\",\n",
    "    \"time_elapsed\": 5,\n",
    "    \"q1\": 5,\n",
    "    \"q2\": 3,\n",
    "    \"q3\": 2,\n",
    "    \"q4\": 2,\n",
    "    \"q5\": 4,\n",
    "    \"freetext\": \"I had a very bad day today...\"\n",
    "}\n",
    "\n",
    "# Invoke lambda function\n",
    "r = aws_lambda.invoke(FunctionName='a5_lambda',\n",
    "                      InvocationType='RequestResponse',\n",
    "                      Payload=json.dumps(p1_response_1))\n",
    "json.loads(r['Payload'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'statusCode': 400, 'body': 'Invalid entry. Not enough time elapsed.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(10)\n",
    "\n",
    "p1_response_2 = {\n",
    "    \"user_id\": \"0001\",\n",
    "    \"timestamp\": \"092821120001\",\n",
    "    \"time_elapsed\": 2,\n",
    "    \"q1\": 5,\n",
    "    \"q2\": 3,\n",
    "    \"q3\": 2,\n",
    "    \"q4\": 2,\n",
    "    \"q5\": 4,\n",
    "    \"freetext\": \"I had a very bad day today...\"\n",
    "}\n",
    "\n",
    "# Invoke lambda function\n",
    "r = aws_lambda.invoke(FunctionName='a5_lambda',\n",
    "                      InvocationType='RequestResponse',\n",
    "                      Payload=json.dumps(p1_response_2))\n",
    "json.loads(r['Payload'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'statusCode': 200,\n",
       " 'body': 'OK. DynamoDB table updated and response uploaded to S3'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(10)\n",
    "\n",
    "p1_response_3 = {\n",
    "    \"user_id\": \"0001\",\n",
    "    \"timestamp\": \"093021120300\",\n",
    "    \"time_elapsed\": 5,\n",
    "    \"q1\": 1,\n",
    "    \"q2\": 1,\n",
    "    \"q3\": 2,\n",
    "    \"q4\": 2,\n",
    "    \"q5\": 2,\n",
    "    \"freetext\": \"I lost my car keys this afternoon at lunch, so I'm more stressed than normal\"\n",
    "}\n",
    "# Invoke lambda function\n",
    "r = aws_lambda.invoke(FunctionName='a5_lambda',\n",
    "                      InvocationType='RequestResponse',\n",
    "                      Payload=json.dumps(p1_response_3))\n",
    "json.loads(r['Payload'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'statusCode': 200,\n",
       " 'body': 'OK. DynamoDB table updated and response uploaded to S3'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(10)\n",
    "\n",
    "p2_response_1 = {\n",
    "    \"user_id\": \"0002\",\n",
    "    \"timestamp\": \"092821120000\",\n",
    "    \"time_elapsed\": 5,\n",
    "    \"q1\": 4,\n",
    "    \"q2\": 1,\n",
    "    \"q3\": 1,\n",
    "    \"q4\": 1,\n",
    "    \"q5\": 3,\n",
    "    \"freetext\": \"I'm having a great day!\"\n",
    "}\n",
    "\n",
    "# Invoke lambda function\n",
    "r = aws_lambda.invoke(FunctionName='a5_lambda',\n",
    "                      InvocationType='RequestResponse',\n",
    "                      Payload=json.dumps(p2_response_1))\n",
    "json.loads(r['Payload'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'statusCode': 200,\n",
       " 'body': 'OK. DynamoDB table updated and response uploaded to S3'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "time.sleep(10)\n",
    "\n",
    "p3_response_1 = {\n",
    "    \"user_id\": \"0003\",\n",
    "    \"timestamp\": \"092821120001\",\n",
    "    \"time_elapsed\": 5,\n",
    "    \"q1\": 1,\n",
    "    \"q2\": 3,\n",
    "    \"q3\": 3,\n",
    "    \"q4\": 1,\n",
    "    \"q5\": 4,\n",
    "    \"freetext\": \"It was a beautiful, sunny day today.\"\n",
    "}\n",
    "\n",
    "# Invoke lambda function\n",
    "r = aws_lambda.invoke(FunctionName='a5_lambda',\n",
    "                      InvocationType='RequestResponse',\n",
    "                      Payload=json.dumps(p3_response_1))\n",
    "json.loads(r['Payload'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'statusCode': 200,\n",
       " 'body': 'OK. DynamoDB table updated and response uploaded to S3'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(10)\n",
    "\n",
    "p4_response_1 = {\n",
    "    \"user_id\": \"0004\",\n",
    "    \"timestamp\": \"092821120002\",\n",
    "    \"time_elapsed\": 8,\n",
    "    \"q1\": 1,\n",
    "    \"q2\": 1,\n",
    "    \"q3\": 1,\n",
    "    \"q4\": 1,\n",
    "    \"q5\": 1,\n",
    "    \"freetext\": \"I had a very bad day today...\"\n",
    "}\n",
    "\n",
    "# Invoke lambda function\n",
    "r = aws_lambda.invoke(FunctionName='a5_lambda',\n",
    "                      InvocationType='RequestResponse',\n",
    "                      Payload=json.dumps(p4_response_1))\n",
    "json.loads(r['Payload'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'statusCode': 200,\n",
       " 'body': 'OK. DynamoDB table updated and response uploaded to S3'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(10)\n",
    "\n",
    "p5_response_1 = {\n",
    "    \"user_id\": \"0005\",\n",
    "    \"timestamp\": \"092821122000\",\n",
    "    \"time_elapsed\": 5,\n",
    "    \"q1\": 3,\n",
    "    \"q2\": 3,\n",
    "    \"q3\": 3,\n",
    "    \"q4\": 3,\n",
    "    \"q5\": 3,\n",
    "   \"freetext\": \"I'm feeling okay, but not spectacular\"\n",
    "}\n",
    "\n",
    "# Invoke lambda function\n",
    "r = aws_lambda.invoke(FunctionName='a5_lambda',\n",
    "                      InvocationType='RequestResponse',\n",
    "                      Payload=json.dumps(p5_response_1))\n",
    "json.loads(r['Payload'].read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "Print a list of the objects in your S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0001092821120000.json', '0001093021120300.json', '0002092821120000.json', '0003092821120001.json', '0004092821120002.json', '0005092821122000.json']\n"
     ]
    }
   ],
   "source": [
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List S3 objects\n",
    "response = s3.list_objects(Bucket=\"mariagabrielaa-a5\")\n",
    "lst_objects = []\n",
    "for obj in response['Contents']:\n",
    "    lst_objects.append(obj['Key'])\n",
    "\n",
    "print(lst_objects)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "Using a SQL-style query, Query your DynamoDB database to identify all user_ids with a Likert scale response of at least 3 on q2 or q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'user_id': '0005'}, {'user_id': '0003'}]\n"
     ]
    }
   ],
   "source": [
    "response = table.meta.client.execute_statement(\n",
    "    Statement='''\n",
    "              SELECT user_id\n",
    "              FROM a5_results\n",
    "              WHERE q2 >= 3 OR q4 >= 3\n",
    "              '''\n",
    ")\n",
    "\n",
    "item = response['Items']\n",
    "print(item)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "Get the DynamoDB data associated with user_id ‘0001’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q1': Decimal('1'), 'q2': Decimal('1'), 'user_id': '0001', 'q3': Decimal('2'), 'q4': Decimal('2'), 'q5': Decimal('2'), 'num_submission': Decimal('2'), 'freetext': \"I lost my car keys this afternoon at lunch, so I'm more stressed than normal\"}\n"
     ]
    }
   ],
   "source": [
    "response = table.get_item(\n",
    "        Key={'user_id': '0001'}\n",
    "        )\n",
    "print(response['Item'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) \n",
    "\n",
    "#### Discuss the relative merits of performing partition key lookups (as in (c)) versus more complex analytical queries (as in part (b)) in a DynamoDB database. What is another type of scalable database that we talked about in class that might be a better option if we needed to perform a lot of relatively small, fast analytical queries (as in (b)) on this data (assuming we still need to perform many small insertions and updates as participants continue to submit surveys)? Why might this be a better option?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamo DB partitions the data by using the partition key, in this case `user_id`. This makes partition look ups like in **part c** really fast and efficient. One limitation though is that these lookups can only retrieve data associated with a single partition key, so it is a less suitable approach for performing more complex queries where we can analyze data across several attributes at the same time, like we do in **part b**.\n",
    "\n",
    "Dynamo DB can handle SQL-type more complex analytical queries (as in **part b**), but only to a degree of complexity. If the query is very complex or the data is very large, then Dynamo DB will not be efficient at it. Compared to partition-key look-ups, SQL queries are generally slower. In this case, the data we are using is very, very small so performance difference between the two approaches is not noticeable. \n",
    "\n",
    "A NoSQL solution like DynamoDB is great if the data is unstructured/semi-structured or there are no clear relations between the data. It is also a good option if the purpose is to perform many insertions/retrieve items, like in this case, where we are inserting survey submissions as participants respond to a survey in real time. The reason for this is that Dynamo DB has high **throughput**, which allows for many concurrent reads and writes. \n",
    "\n",
    "It is also important to mention that Dynamo DB prioritizes **availability** by default. This is a limitation because when we perform a query, we might not get back the latest version. It guarantees **eventual consistentcy**, which is good enough if are only performing a few spaced-out queries, but it is not ideal if we have many concurrent I/O's.\n",
    "\n",
    "However, if we want to perform a lot of relatively small and fast analytical queries (like in **part b**), then a better option is a **Relational Database** solution like:\n",
    "\n",
    "- MySQL\n",
    "- PostgreSQL\n",
    "- AWS Aurora\n",
    "\n",
    "This would be a better option because relational databases are specifically optimized for small analytical SQL queries that are row-centric. Essentially, when we perform a query in a relational database, it goes over _row by row_ to complete the query. This works very well in databases that are not big data (if the data is very large, then Redshift would be the better option). Unlike DynamoDB, Relational Databases emphasize **consistency** by default, which is important when there are many concurrent reads and writes, otherwise when someone is performing a query, they might not get the most up-to-date result.\n",
    "\n",
    "In the case that the data is very large, aka in the petabyte range, then it is preferrable to use Redshift instead of a relational database because it can handle parallel queries across a cluster of nodes and can perform faster queries by _columns_ instead of iterating over entire rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tear down cloud architecture\n",
    "remove_objects('mariagabrielaa-a5')\n",
    "delete_table(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
