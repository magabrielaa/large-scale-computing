# A2

## Question 1
- I created a `.py` file called [q1.py](https://github.com/macs30113-s23/a2-magabrielaa/blob/main/q1.py) that contains `numba` AOT pre-compilation code. The script also imports the pre-compiled module and runs the nested loop portion of the simulation that is parallelizable
- I also generated a `sbatch` script called [q1.sbatch](https://github.com/macs30113-s23/a2-magabrielaa/blob/main/q1.sbatch) that requests a single core
and runs `q1.py` using the Message Passing Interface (MPI) standard. Since it is only 1 core, it runs as a single process.
- The AOT pre-compiled module generated is [compiled_aot.cpython-37m-x86_64-linux-gnu.so](https://github.com/macs30113-s23/a2-magabrielaa/blob/main/compiled_aot.cpython-37m-x86_64-linux-gnu.so)
- I used the `time.time()` package to time how long it takes to run the pre-compiled AOT version
    - I set the beginning of the time right after importing the pre-compiled module named `compiled_aot`
    - **Computation time:** 0.2341010570526123 seconds. This is recorded in the `q1.sbatch` output file titled [q1.out](https://github.com/macs30113-s23/a2-magabrielaa/blob/main/q1.out)


## Question 2

### 2.a

- The parallelized simulation is contained in [q2.py](https://github.com/macs30113-s23/a2-magabrielaa/blob/main/q2.py)
    - Here, I set the number of simulations `n_runs` to 1.000
    - I redefine S, the simulations parameter, so that the number of simulation runs across MPI processes is evenly distributed:
    ```python
    S = int(n_runs/size)
    ```
    - Also, I set the random seed to the rank for each process:
    ```python
    np.random.seed(rank)
    ```
    - I make use of the pre-compiled AOT module to run the simulation:
    ```python
    z_mat_aot = compiled_aot.loop_aot(S, T, z_0, eps_mat, rho, mu, z_mat)
    ```
    - I use `comm.Gather` to gather all z_mat arrays at the root node so that, should further analysis be made, a single MPI process (rank = 0) contains the complete z_mat array
    - I print out the computation times and number of cores per process

### 2.b

- In the `sbatch` file [q2.sbatch](https://github.com/macs30113-s23/a2-magabrielaa/blob/main/q2.sbatch), I loop through the number of cores (from 1 to 20) to repeatedly run the 1.000 simulations. I set the number of cores to 20, ie. `ntasks=20`.
    - The higher the number of cores, the lower the number of simulations processed per core, ie. at 1 core, the single core handles the complete 1.000 simulations vs. at 20 cores, each core handles 50 simulations.
- The output containing the computation timings and number of cores per run is called [q2.out](https://github.com/macs30113-s23/a2-magabrielaa/blob/main/q2.out)

### 2.c
- The python script that produces the plot is [plot.py](https://github.com/macs30113-s23/a2-magabrielaa/blob/main/plot.py) and it essentially reads the `q2.out` file generated by `q2.sbatch` and retrieves the computation times and number of cores for each run.
- As a result, I get the following plot:
![Parallelization Plot](https://github.com/macs30113-s23/a2-magabrielaa/blob/main/plot.png)

#### Analysis
In the context of this exercise, the size of the problem is **fixed** because we are running 1.000 simulations, rather than increasing the number of simulations as we increase the number of cores.

As such, my expectations from Assignment 1, Question 2 were that the theoretical upper-bound would be dictated by **Amdahl's Law**, which states that the amount of parallelization we can achieve is subject to the portion of the code that is serial. Therefore, I expected to see an initial speedup as the number of cores increase, followed by a pleatau in computation speed after a certain point. This is because, no matter how many cores are added, there is a limit to speedup gains based on the part of the code that is serial. 

From the plot, we can observe that, in general, as the number of cores (ie. MPI processes) _increases_, the computation time _decreases_. This is particularly the case at the _beginning_ of the chart, where the first 7 cores show a signficant **marked decline in computation time**. After this point, the level of speedup varies, at times increasing or decreasing with three observable peaks than range from 0.10 to 0.05 seconds. After 17 cores, it appears that the computation speedup reaches a plateau.

In the range of these 20 runs, the lowest computation time for 1.000 simulations is achieved at 9 cores, which results in a computation time of  **0.044021 seconds**. If we take the lowest computation speed achieved in this exercise (ie. 0.044 seconds) and compare it to the AOT simulation from Question 1 which ran in **0.234101 seconds**, the percentage change is a **81.2% decrease**, which is a considerable speedup improvement.

In _Assignment 1, Question 2_, I timed the serial and parallel portion of the code and got that 5% of the code was serial and 95% parallelizable. The theoretical speedup upper-limit with Amdalh's Law for 10 processors is 6.89. Following the same proportions, the theoretical speedup upper-limit for 20 processors is 10.25:

```python
s = 0.05
p = 0.95
print("Amdalh speed up with 20 processors:", 1 / ((s) + (p/20)))

Amdalh speed up with 20 processors: 10.256410256410255
```
I can also calculate the factor by which execution time was reduced from the **AOT-only** version in Question 1 to the **AOT + MPI** version in Question 2:

```python
factor = 0.234101 / 0.044021
print("The execution time was reduced by a factor of approximately ", factor)

The execution time was reduced by a factor of approximately  5.317939165398333
```
Therefore, in this case the computation time was reduced by a factor of 5.31, which is within the limits of Amdahl's theoretical upper-limit. Overall, the chart is consistent with Amdahl's Law, as expected. Since the speedup is greater at the beginning (with the introduction of the first 7 cores) and then varies up to a point where it begins to stabilize (from core 17 onwards), we can expect that the probability of achieving further speedup by further increasing the number of cores will be lower.

One of the reasons that we might observe the peaks in computation time variability might be due to **communication overhead**. As we increase the number of nodes, communications between nodes can become a bottleneck. In my code, I am using comm.Gather to consolidate z_mat arrays into a single array at the root, which will likely result in higher overhead as the number of nodes increase, eventually leading to a stable level of computation time.

Another reason might be **contention for resources** between nodes, such as **memory bandwidth**. Each node is performing a given proportion of simulations, where the generated z_mat simulation arrays need to be stored in memory. As we increase the number of cores, at one point, nodes might be competing for this resource, resulting in limited speedups. It could also be the case that as other Midway users are running jobs, some of the MPI processes might become slower if there is contention for resources. This will depend on the hardware configuration used and the user traffic on the Midway.

In `q2.sbatch`, I use the `broadwl` partition, which has the following characteristics in the [Midway cluster](https://rcc.uchicago.edu/documentation/_build/html/whats-new/index.html):
- CPU Cores per Node: 28 x Intel E5-2680v4 @ 2.40 GHz

I also constraint the nodes to FDR's, which have these characteristics:
- CPU: 2 * Intel E5-2680 v4  2.40 GHz
- Memory:  64 GB TruDDR4  2.40 GHz
- Interconnect:  InfiniBand FDR 56 Gb/s

Even though I use comm.Gather to consolidate z_mat arrays at the root, in my code there are no further communications between nodes, such as passing and receiving information or broadcasting. For this reason, FDR nodes are well equipped to handle this parallelization. If there were more communications between nodes, then a change in hardware to EDR nodes (which have a higher interconnect speed than FDR nodes) might become necessary.

The amount of memory per node, 64 GB, also seems to be sufficient for this exercise, otherwise we would be observing more dramatic increases in computation speed. Each node has 2 physical processors Intel E5-2680 v4 and each processor has 14 cores, making a total of 28 CPU cores per node. This means that each node can run 28 parallel simulations and since we are running a maximum of 20 parallel simulations, there is plenty of capacity available.