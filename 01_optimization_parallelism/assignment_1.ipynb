{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1"
      ],
      "metadata": {
        "id": "ItxFP5fziF7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (a) \n",
        "**Rewrite computationally intensive portion of the code as a separate function and compile it using @jit decorator. Then compare the time it takes to run the original version vs. the jit-accelerated one.**"
      ],
      "metadata": {
        "id": "aQK6nhl2T20h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as sts\n",
        "\n",
        "# Set model parameters\n",
        "rho = 0.5\n",
        "mu = 3.0\n",
        "sigma = 1.0\n",
        "z_0 = mu\n",
        "\n",
        "# Set simulation parameters, draw all idiosyncratic random shocks,\n",
        "# and create empty containers\n",
        "S = 1000  # Set the number of lives to simulate\n",
        "T = 4160  # Set the number of periods for each simulation\n",
        "np.random.seed(25)\n",
        "eps_mat = sts.norm.rvs(loc=0, scale=sigma, size=(T, S))\n",
        "z_mat = np.zeros((T, S))\n",
        "\n",
        "def loop (S,T,z_0,eps_mat,rho,mu,z_mat):\n",
        "  for s_ind in range(S):\n",
        "      z_tm1 = z_0\n",
        "      for t_ind in range(T):\n",
        "          e_t = eps_mat[t_ind, s_ind]\n",
        "          z_t = rho * z_tm1 + (1 - rho) * mu + e_t\n",
        "          z_mat[t_ind, s_ind] = z_t\n",
        "          z_tm1 = z_t\n",
        "  return z_mat"
      ],
      "metadata": {
        "id": "bE-w2jV6MJTl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxAlfqALLdvD"
      },
      "outputs": [],
      "source": [
        "from numba import jit\n",
        "import numpy as np\n",
        "import scipy.stats as sts\n",
        "\n",
        "# (a) Rewrite computationally intensive portion of the code as a separate \n",
        "# function and compile it using the numba @jit decorator\n",
        "@jit(nopython=True)\n",
        "def loop_jit (S,T, z_0,eps_mat, rho, mu, z_mat):\n",
        "  for s_ind in range(S):\n",
        "    z_tm1 = z_0\n",
        "    for t_ind in range(T):\n",
        "        e_t = eps_mat[t_ind, s_ind]\n",
        "        z_t = rho * z_tm1 + (1 - rho) * mu + e_t\n",
        "        z_mat[t_ind, s_ind] = z_t\n",
        "        z_tm1 = z_t\n",
        "  return z_mat\n",
        "\n",
        "# Incorporate the function into the program above and compare how long it takes\n",
        "# to run the original version of the code vs the time it takes to run your function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Original version time\n",
        "z_mat = np.zeros((T, S))\n",
        "%time loop(S,T,z_0,eps_mat,rho,mu, z_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vh6y-dLQQNh",
        "outputId": "b4db6dbd-1b6f-4537-fbc6-5dff931712e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.21 s, sys: 3.85 ms, total: 3.21 s\n",
            "Wall time: 3.4 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.22827309, 4.0268903 , 2.16041515, ..., 3.5980616 , 4.2172622 ,\n",
              "        4.27401251],\n",
              "       [2.72415509, 2.76100373, 1.84533647, ..., 4.56294503, 4.15767785,\n",
              "        2.40206165],\n",
              "       [0.88323972, 3.60232523, 1.86360091, ..., 3.68876144, 3.25430265,\n",
              "        3.55861037],\n",
              "       ...,\n",
              "       [5.74366912, 2.74903461, 3.7073953 , ..., 1.87117226, 1.64628771,\n",
              "        2.43676356],\n",
              "       [5.44468565, 3.16764667, 4.44006409, ..., 2.86687192, 3.29363719,\n",
              "        2.6143125 ],\n",
              "       [3.72185889, 4.1198887 , 4.67028049, ..., 2.06330704, 3.21297341,\n",
              "        2.6348468 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Jit version FIRST time will include compilation time\n",
        "z_mat = np.zeros((T, S))\n",
        "%time loop_jit(S,T,z_0,eps_mat,rho,mu, z_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHh4s0v_fGP4",
        "outputId": "0f7ae26f-d3b3-4a9c-c107-95da31cfdecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 222 ms, sys: 1.89 ms, total: 224 ms\n",
            "Wall time: 225 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.22827309, 4.0268903 , 2.16041515, ..., 3.5980616 , 4.2172622 ,\n",
              "        4.27401251],\n",
              "       [2.72415509, 2.76100373, 1.84533647, ..., 4.56294503, 4.15767785,\n",
              "        2.40206165],\n",
              "       [0.88323972, 3.60232523, 1.86360091, ..., 3.68876144, 3.25430265,\n",
              "        3.55861037],\n",
              "       ...,\n",
              "       [5.74366912, 2.74903461, 3.7073953 , ..., 1.87117226, 1.64628771,\n",
              "        2.43676356],\n",
              "       [5.44468565, 3.16764667, 4.44006409, ..., 2.86687192, 3.29363719,\n",
              "        2.6143125 ],\n",
              "       [3.72185889, 4.1198887 , 4.67028049, ..., 2.06330704, 3.21297341,\n",
              "        2.6348468 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Jit version after initial compilation\n",
        "z_mat = np.zeros((T, S))\n",
        "%time loop_jit(S,T,z_0,eps_mat,rho,mu, z_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgMGxc08QRqD",
        "outputId": "a6ae064a-2d65-4863-bbd3-0364d0ce5717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 51.8 ms, sys: 0 ns, total: 51.8 ms\n",
            "Wall time: 53 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.22827309, 4.0268903 , 2.16041515, ..., 3.5980616 , 4.2172622 ,\n",
              "        4.27401251],\n",
              "       [2.72415509, 2.76100373, 1.84533647, ..., 4.56294503, 4.15767785,\n",
              "        2.40206165],\n",
              "       [0.88323972, 3.60232523, 1.86360091, ..., 3.68876144, 3.25430265,\n",
              "        3.55861037],\n",
              "       ...,\n",
              "       [5.74366912, 2.74903461, 3.7073953 , ..., 1.87117226, 1.64628771,\n",
              "        2.43676356],\n",
              "       [5.44468565, 3.16764667, 4.44006409, ..., 2.86687192, 3.29363719,\n",
              "        2.6143125 ],\n",
              "       [3.72185889, 4.1198887 , 4.67028049, ..., 2.06330704, 3.21297341,\n",
              "        2.6348468 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results in one iteration:\n",
        "\n",
        "When using **%time**,which measures the time it takes to run the function in one single iteration, I get that the original version runs in \n",
        "**3.21 s**. In comparison, the jit-version takes **224 ms** the first time. This is already faster than the original version. However, it should be noted that this first timing includes the compilation time.\n",
        "\n",
        "When I run the jit-version a second time, it runs in **51.8 ms**, much faster now that it is precompiled."
      ],
      "metadata": {
        "id": "T8UXAgXeTgdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, run the original version and time it in several iterations\n",
        "z_mat = np.zeros((T, S))\n",
        "%timeit loop(S,T,z_0,eps_mat,rho,mu, z_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c163UG-bfyWB",
        "outputId": "ed6fc01c-de85-403e-ecbe-54bb0a871ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.71 s ± 638 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Likewise, time jit-accelerated version with several iterations\n",
        "z_mat = np.zeros((T, S))\n",
        "%timeit loop_jit(S,T,z_0,eps_mat,rho,mu, z_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJBTaIWwf4u1",
        "outputId": "7b932150-57ac-4fea-ae16-78e931b47cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83.4 ms ± 3.89 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results in multiple iterations:\n",
        "\n",
        "After running the original and jit-accelerated versions with **%timeit**, which times across multiple runs, I get the following results\n",
        "\n",
        "The original version runs in **3.71 s** vs. **83.4 ms** with the jit-accelerated version. This is a considerable speed-up, which shows that when we have NumPy and for loops, Numba performs best as a compiler and gives us efficiency gains.\n"
      ],
      "metadata": {
        "id": "cnNiHIb8gxxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (b)\n",
        "\n",
        "**Pre-compile a version of the function ahead of time using numba. Incorporate this pre-compiled code into the program above and compare how long it takes to run the original version of the code (as it is written above) with the time it takes to run your pre-compiled version.**"
      ],
      "metadata": {
        "id": "9uxTtKFOUOn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numba.pycc import CC\n",
        "\n",
        "# name of compiled module to create:\n",
        "cc = CC('compiled_aot')\n",
        "\n",
        "# name of function in module, with explicit data types required\n",
        "@cc.export('loop_aot', 'f8[:, :](i4, i4, f8, f8[:, :], f8, f8, f8[:, :])')\n",
        "def loop_aot(S, T, z_0, eps_mat, rho, mu, z_mat):\n",
        "  for s_ind in range(S):\n",
        "    z_tm1 = z_0\n",
        "    for t_ind in range(T):\n",
        "        e_t = eps_mat[t_ind, s_ind]\n",
        "        z_t = rho * z_tm1 + (1 - rho) * mu + e_t\n",
        "        z_mat[t_ind, s_ind] = z_t\n",
        "        z_tm1 = z_t\n",
        "  return z_mat\n",
        "\n",
        "cc.compile()"
      ],
      "metadata": {
        "id": "7YAMi0ltT0f_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pre-compiled module\n",
        "import compiled_aot\n",
        "\n",
        "# Set model and simulation parameters\n",
        "rho = 0.5\n",
        "mu = 3.0\n",
        "sigma = 1.0\n",
        "z_0 = mu\n",
        "S = 1000\n",
        "T = 4160 \n",
        "np.random.seed(25)\n",
        "eps_mat = sts.norm.rvs(loc=0, scale=sigma, size=(T, S))\n",
        "z_mat = np.zeros((T, S))\n",
        "\n",
        "# Time compiled version with several iterations\n",
        "%timeit compiled_aot.loop_aot(S, T, z_0, eps_mat, rho, mu, z_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B9Jx5Xfdp70",
        "outputId": "a94b8761-2e72-4ab2-a7d2-568815c98ed4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31.3 ms ± 477 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results from pre-compiled code:\n",
        "\n",
        "Using %timeit, I get that the pre-compiled version runs in **31.3 ms**, compared to **3.71 s** (ie. 3710 ms) with the original version. The pre-compiled version is therefore 3678.7 ms faster than the original version, a considerable speedup."
      ],
      "metadata": {
        "id": "s-s8a2tJmClb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (c)\n",
        "\n",
        "**How does the pre-compiled code speedup compare to the @jit speedup? With this particular simulation application in mind, what contexts might it make sense to precompile this code ahead of time as opposed to using @jit?**"
      ],
      "metadata": {
        "id": "FD_lDItSh4HA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that the original version, as measured with %timeit, runs in 3.71 s (ie. **3710 ms**):\n",
        "<br>\n",
        "<br> **Speedup difference between original and @jit version**\n",
        "<br> = 83.4 ms - 3710 ms = **-3626.6 ms**\n",
        "<br>\n",
        "<br> **Speedup difference between original and pre-compiled AOT version**\n",
        "<br> = 31.3 ms - 3710 ms = **-3678.7 ms**\n",
        "\n",
        "<br>\n",
        "The pre-compiled version is faster than the @jit version by **52.1 ms** (ie. 31.3 ms - 83.4 ms). If we calculate the percentage difference, this is equivalent to 83.9%, that is to say, the pre-compiled AOT version is 90.85% faster than the @jit version. One of the reasons that ahead-of-time compilation is faster than just-in-time is because the data types are explicitly specified before the compiling takes place. In AOT, the signature needs to specify the data types, which allows the compiler to create machine code that is optimized for the specific data types that will be processed **ahead of time**. In contrast, @jit infers the data types at runtime, which is relatively slower (compared to AOT),but gives more flexibility for cases where the data types might change through the execution of the code.\n",
        "<br>\n",
        "<br>\n",
        "It makes sense to use @jit when we are in a production mode that is subject to constant changes/adjustments in the code. In the AOT case, we have to pre-compile a new module each time we make a change in the compilation code, which takes additional time. Once the code is set, using the AOT is beneficial because it is faster and it needs to be pre-compiled only once.\n",
        "<br>\n",
        "<br>\n",
        "If the data types change during the execution, then it is preferrable to use @jit because it infers the type dynamically, whereas with AOT, the types are fixed ahead of the compilation and cannot be changed during the execution.\n",
        "<br>\n",
        "<br>\n",
        "The AOT compilation module that is generated is specific to a particular architecture and OS so the module cannot simply be shared between architectures as it will not work. If there is a need to share code, then using @jit would be better.\n",
        "<br>\n",
        "<br>\n",
        "If the size of the problem is very large, with a big number of simulations, then the AOT compiler is a better choice as it has more speedup than @jit."
      ],
      "metadata": {
        "id": "MSbFe5fziCnp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2"
      ],
      "metadata": {
        "id": "G_1zL82niKI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Describe the portions of the above code that are potentially parallelizable and the overall theoretical speedup you might expect by parallelizing the code in these spots. Do you expect a linear speedup as you increase parallelism (e.g. from 1 process to 10 processes to 100 processes)?**\n",
        "\n",
        "*Note: Assume that the generation of eps_mat via sts.norm.rvs() cannot be parallelized and must occur on a single process.*"
      ],
      "metadata": {
        "id": "5UFERIyyiSJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*  The model parameters are fixed values, so they **cannot be parallelized**.\n",
        "\n",
        "  * rho = 0.5\n",
        "  * mu = 3.0\n",
        "  * sigma = 1.0\n",
        "  * z_0 = mu\n",
        "\n",
        "* In the simulation parameters: S, T, the random seed, and the initial z_mat (which is a zero matrix) are all fixed/given at the beginning of the model and therefore **cannot be parallelized** as they happen in one single moment.\n",
        "\n",
        "  * S = 1000\n",
        "  * T = 4160 \n",
        "  * np.random.seed(25)\n",
        "  * z_mat = np.zeros((T, S))\n",
        "\n",
        "* We are assuming that the generation of eps_mat **cannot be parallelized** and must occurr in a single process.\n",
        "  * eps_mat = sts.norm.rvs(loc=0, scale=sigma, size=(T, S))\n",
        "\n",
        "* The nested for loop can be **parallelized** because each iteration in the loop is independent from one another and there are no dependencies between runs. This is an example of a Monte Carlo simulation, which is parallelizable.\n"
      ],
      "metadata": {
        "id": "tEudfG8xqju6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXPECTATION\n",
        "\n",
        "I expect the theoretical speedup to be dictated by Amdalh's law rather than Gustafson's because Amdalh assumes the size of the problem is fixed.\n",
        "\n",
        "In our Monte Carlo simulation, we have a **fixed problem size** because we are running a set/fixed number of 1000 lifetimes of health shocks, with each lifetime equivalent to 4160 weeks. Therefore, the theoretical speedup for this application, as it is currently defined, will not be linear. Below I calculate the theoretical speedups using both laws."
      ],
      "metadata": {
        "id": "hjlKpIw78SyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### THEORETICAL SPEED UP CALCULATIONS\n",
        "Since only the nested for loop can be parallelized, to calculate the theoretical speed-ups, I need to know the approximate percentage of the time that is run by the serial vs. potentially parallelizable code."
      ],
      "metadata": {
        "id": "rDUmMky9tcwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (1) Create function that contains the serial portion of the code\n",
        "\n",
        "def serial_parameters():\n",
        "  # Set model parameters\n",
        "  rho = 0.5\n",
        "  mu = 3.0\n",
        "  sigma = 1.0\n",
        "  z_0 = mu\n",
        "\n",
        "  # Set simulation parameters, draw all idiosyncratic random shocks,\n",
        "  # and create empty containers\n",
        "  S = 1000  # Set the number of lives to simulate\n",
        "  T = 4160  # Set the number of periods for each simulation\n",
        "  np.random.seed(25)\n",
        "  eps_mat = sts.norm.rvs(loc=0, scale=sigma, size=(T, S))\n",
        "  z_mat = np.zeros((T, S))\n",
        "\n",
        "# (2) Create function that contains the potentially-parallelizable portion\n",
        "# of the code\n",
        "def parallel_loop (S,T,z_0,eps_mat,rho,mu,z_mat):\n",
        "  for s_ind in range(S):\n",
        "      z_tm1 = z_0\n",
        "      for t_ind in range(T):\n",
        "          e_t = eps_mat[t_ind, s_ind]\n",
        "          z_t = rho * z_tm1 + (1 - rho) * mu + e_t\n",
        "          z_mat[t_ind, s_ind] = z_t\n",
        "          z_tm1 = z_t\n",
        "  return z_mat"
      ],
      "metadata": {
        "id": "FqA-wXr6t1AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Time serial portion of the code\n",
        "%timeit serial_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SlTWx-MuOfO",
        "outputId": "bdd4a1c3-0e2c-4aa6-b0ac-de3cab14e3a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "176 ms ± 22.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Time potentially parallelizable portion of the code\n",
        "%timeit parallel_loop (S,T,z_0,eps_mat,rho,mu,z_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuCQRABAudy1",
        "outputId": "e7a8d01e-982b-40eb-d708-4897f7b0d64a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.56 s ± 517 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total run-time for the simulation is given by:\n",
        "<br> S + P = 1 = 100%\n",
        "<br> 176 ms + 3560 ms = 3736 ms\n",
        "\n",
        "Then, the serial and parallelizable percentages are:\n",
        "<br> **Serial %**= 176 ms / 3736 ms = 0.050 = 5%\n",
        "<br> **Parallel %** = 3560 ms / 3736 ms = 0.95 = 95%\n"
      ],
      "metadata": {
        "id": "DYIguI0xul4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AMDALH'S LAW\n",
        "\n",
        "Calculates the potential speedup based on the amount of the code that is parallel. It estimates the theoretical speedup of a fixed-sized problem as the number of processors increase. \n",
        "\n",
        "**Formula:** Speedup(N) = 1 / (S + P/N)"
      ],
      "metadata": {
        "id": "arGJmSeoweY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = 0.05\n",
        "p = 0.95\n",
        "# Calculate Amdalh's speed up with 1 processor\n",
        "print(\"1 processor Amdalh speed up\", 1 / ((s) + (p/1)))\n",
        "\n",
        "# Calculate Amdalh's speed up with 10 processors\n",
        "print(\"10 processor Amdalh speed up\", 1 / ((s) + (p/10)))\n",
        "\n",
        "# Calculate Amdalh's speed up with 100 processors\n",
        "print(\"100 processor Amdalh speed up\", 1 / ((s) + (p/100)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC3LB-8lwv4M",
        "outputId": "1bfaa48f-e51c-48eb-d2aa-207f006a35be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 processor Amdalh speed up 1.0\n",
            "10 processor Amdalh speed up 6.896551724137931\n",
            "100 processor Amdalh speed up 16.806722689075627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GUSTAFSON'S LAW\n",
        "\n",
        "Calculates the potential speedup if the size of the problem grows proportionally to the number of processors. It states that parallel code runs should increase the size of the problem as more processors are added.\n",
        "\n",
        "\n",
        "**Formula:** Speedup(N) = N - S * (N - 1)"
      ],
      "metadata": {
        "id": "V-_iHzHex7pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = 0.05\n",
        "p = 0.95\n",
        "\n",
        "# Calculate Gustafson's speed up with 1 processor\n",
        "print(\"1 processor Gustafson speed up\", 1 - s * (1 - 1))\n",
        "\n",
        "# Calculate Gustafson's speed up with 10 processors\n",
        "print(\"10 processor Gustafson speed up\", 10 - s * (10 - 1))\n",
        "\n",
        "# Calculate Gustafson's speed up with 10 processors\n",
        "print(\"100 processor Gustafson speed up\", 100 - s * (100 - 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgx4ROy-ymb0",
        "outputId": "21f2f0db-ccd0-4981-89bb-147bc43a3551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 processor Gustafson speed up 1.0\n",
            "10 processor Gustafson speed up 9.55\n",
            "100 processor Gustafson speed up 95.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CONCLUSION\n",
        "\n",
        "With Amdalh's Law, as expected, there is no linear speedup because 5% of the code is serial. This means that there is some speedup as we increase the number of of processors, but the speed performance gains we get in return are **not linear** as we increase the number of processors. That is, the speed up goes from 1 to 6.9 to 16.8 as we increase from 1 to 10 to 100 processors. No matter how fast the parallel portion of the code is, we are limited by the serial portion so there is a cap in the speedup we can gain. \n",
        "\n",
        "This is likely to be the theoretical speedup for this particular application, because the **size of the problem is fixed**: 1000 lifetimes of health shocks, each lifetime equivalent to 4160 weeks.\n",
        "\n",
        "In contrast, with Gustafson's Law, the theoretical speedup is linear as we increase the number of processors. The potential speedup increases from 1 to 9.6 to 95.1 as we increase from 1 to 10 to 100 processors. This tells us that a larger problem can be solved in the same amount of time if we have more processors. Here, the speedup is entirely dependent on the number of processors N. If we were to increase the number of simulations in our application, then the theoretical speedup would be dictated by Gustafson's Law."
      ],
      "metadata": {
        "id": "crVDWw9KztSb"
      }
    }
  ]
}