{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6\n",
    "\n",
    "## Question 1 (a)\n",
    "\n",
    "### Write a Python function `send_survey` (which you can assume will be installed with the mobile app and will automatically be invoked after a survey is saved as a JSON file on the device):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "def send_survey(survey_path, sqs_url):\n",
    "    '''\n",
    "    Input: survey_path (str): path to JSON survey data\n",
    "        (e.g. `./survey.json')\n",
    "        sqs_url (str): URL for SQS queue\n",
    "    Output: StatusCode (int): indicating whether the survey\n",
    "            was successfully sent into the SQS queue (200) or not (400)\n",
    "    '''\n",
    "    f = open (survey_path, \"r\")\n",
    "    # Read data from file\n",
    "    data = json.loads(f.read())\n",
    "\n",
    "    sqs = boto3.client('sqs')\n",
    "    response = sqs.send_message(QueueUrl=sqs_url,\n",
    "                                MessageBody=json.dumps(data))\n",
    "    \n",
    "    return response['ResponseMetadata']['HTTPStatusCode']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (b)\n",
    "\n",
    "### Create an SQS queue and configure it to act as a trigger for your Lambda function from Assignment 5 (which will process your data and write it to storage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda function created. Lambda ARN:  arn:aws:lambda:us-east-1:589049386593:function:a6\n",
      "SQS queue created. SQS ARN:  arn:aws:sqs:us-east-1:589049386593:a6\n",
      "S3 bucket created. Bucket name:  mariagabrielaa-a6\n",
      "Number of current tables:  0\n",
      "Table creation time:  2023-05-01 01:31:15.001000-05:00\n",
      "\n",
      "SQS -> Lambda -> S3 -> Dynamo DB Architecture has been launched\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "##############          (A) LAUNCH CLOUD ARCHITECTURE         #############\n",
    "###########################################################################\n",
    "\n",
    "#---------------------(1) CREATE LAMBDA FUNCTION--------------------------\n",
    "\n",
    "# Access IAM role to allow Lambda to interact with other AWS resources\n",
    "aws_lambda = boto3.client('lambda')\n",
    "iam_client = boto3.client('iam')\n",
    "role = iam_client.get_role(RoleName='LabRole')\n",
    "\n",
    "# Open zipped directory that contains lambda function\n",
    "with open('a6.zip', 'rb') as f:\n",
    "    lambda_zip = f.read() \n",
    "\n",
    "try:\n",
    "    # If function hasn't yet been created, create it\n",
    "    response = aws_lambda.create_function(\n",
    "        FunctionName='a6',\n",
    "        Runtime='python3.9',\n",
    "        Role=role['Role']['Arn'], \n",
    "        Handler='lambda_function.lambda_handler', \n",
    "        Code=dict(ZipFile=lambda_zip),\n",
    "        Timeout=60\n",
    "    )\n",
    "except aws_lambda.exceptions.ResourceConflictException:\n",
    "    # If function already exists, update it based on zip file contents\n",
    "    response = aws_lambda.update_function_code(\n",
    "        FunctionName='a6',\n",
    "        ZipFile=lambda_zip\n",
    "        )\n",
    "\n",
    "lambda_arn = response['FunctionArn']\n",
    "print(\"Lambda function created. Lambda ARN: \", lambda_arn)\n",
    "\n",
    "#---------------------(2) CREATE SQS QUEUE-------------------------------\n",
    "\n",
    "# Initialize the SQS client\n",
    "sqs = boto3.client('sqs')\n",
    "\n",
    "# Create SQS Queue which will trigger lambda function\n",
    "try:\n",
    "    queue_url = sqs.create_queue(QueueName='a6',\n",
    "                                 Attributes={'VisibilityTimeout': '70'})['QueueUrl'] \n",
    "except sqs.exceptions.QueueNameExists:\n",
    "    queue_url = [url\n",
    "                 for url in sqs.list_queues()['QueueUrls']\n",
    "                 if 'a6' in url][0]\n",
    "    \n",
    "sqs_info = sqs.get_queue_attributes(QueueUrl=queue_url,\n",
    "                                    AttributeNames=['QueueArn'])\n",
    "sqs_arn = sqs_info['Attributes']['QueueArn']\n",
    "\n",
    "print(\"SQS queue created. SQS ARN: \", sqs_arn)\n",
    "\n",
    "\n",
    "#---------------------(3) TRIGGER LAMBDA THROUGH SQS QUEUE----------------------------\n",
    "\n",
    "# Trigger Lambda Function when new messages enter SQS Queue\n",
    "try:\n",
    "    response = aws_lambda.create_event_source_mapping( \n",
    "        EventSourceArn=sqs_arn, \n",
    "        FunctionName='a6',\n",
    "        Enabled=True,\n",
    "        BatchSize=10\n",
    "    )\n",
    "except aws_lambda.exceptions.ResourceConflictException: \n",
    "    es_id = aws_lambda.list_event_source_mappings(\n",
    "        EventSourceArn=sqs_arn,\n",
    "        FunctionName='a6'\n",
    "    )['EventSourceMappings'][0]['UUID']\n",
    "    \n",
    "    response = aws_lambda.update_event_source_mapping(\n",
    "        UUID=es_id,\n",
    "        FunctionName='a6',\n",
    "        Enabled=True,\n",
    "        BatchSize=10\n",
    "    )\n",
    "\n",
    "#---------------------(4) CREATE S3 BUCKET-------------------------------\n",
    "\n",
    "# Initialize s3 client and resource\n",
    "s3 = boto3.client('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "# Create S3 bucket\n",
    "s3.create_bucket(Bucket='mariagabrielaa-a6')\n",
    "\n",
    "# Get the list_buckets response\n",
    "response = s3.list_buckets()\n",
    "\n",
    "# Print each bucket name (to check bucket was created)\n",
    "for bucket in response['Buckets']:\n",
    "    print(\"S3 bucket created. Bucket name: \", bucket['Name'])\n",
    "\n",
    "\n",
    "#---------------------(5) CREATE DYNAMO DB TABLE-------------------------\n",
    "\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "\n",
    "table = dynamodb.create_table(\n",
    "    TableName='a6_results',\n",
    "    KeySchema=[\n",
    "        {\n",
    "            'AttributeName': 'user_id',\n",
    "            'KeyType': 'HASH'\n",
    "        }\n",
    "    ],\n",
    "    AttributeDefinitions=[\n",
    "        {\n",
    "            'AttributeName': 'user_id',\n",
    "            'AttributeType': 'S'\n",
    "        }\n",
    "    ],\n",
    "    ProvisionedThroughput={\n",
    "        'ReadCapacityUnits': 1,\n",
    "        'WriteCapacityUnits': 1\n",
    "    }\n",
    ")\n",
    "\n",
    "# Wait until AWS confirms that table exists before moving on\n",
    "table.meta.client.get_waiter('table_exists').wait(TableName='a6_results')\n",
    "\n",
    "# get data about table (should currently be no items in table)\n",
    "print(\"Number of current tables: \", table.item_count)\n",
    "print(\"Table creation time: \", table.creation_date_time)\n",
    "\n",
    "print()\n",
    "print(\"SQS -> Lambda -> S3 -> Dynamo DB Architecture has been launched\")\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "##############         (B) TEAR DOWN CLOUD ARCHITECTURE       #############\n",
    "###########################################################################\n",
    "\n",
    "#----------------(1) DELETE LAMBDA AND EVENT SOURCE MAPPINGS---------------\n",
    "\n",
    "def delete_lambda(lambda_name):\n",
    "    try:\n",
    "        aws_lambda.delete_function(FunctionName=lambda_name)\n",
    "        print(\"Lambda Function Deleted\")\n",
    "    except aws_lambda.exceptions.ResourceNotFoundException:\n",
    "        print(\"AWS Lambda Function Already Deleted\")\n",
    "\n",
    "    event_source_mappings = aws_lambda.list_event_source_mappings(\n",
    "        FunctionName='a6')['EventSourceMappings']\n",
    "    \n",
    "    for mapping in event_source_mappings:\n",
    "        aws_lambda.delete_event_source_mapping(UUID=mapping['UUID'])\n",
    "        print(\"Event source mapping for 'a6' function deleted\")\n",
    "\n",
    "\n",
    "#----------------------(2) DELETE SQS QUEUE-------------------------------\n",
    "\n",
    "def delete_queue(queue_url):\n",
    "    try:\n",
    "        sqs.delete_queue(QueueUrl=queue_url)\n",
    "        print(\"SQS Queue Deleted\")\n",
    "    except sqs.exceptions.QueueDoesNotExist:\n",
    "        print(\"SQS Queue Already Deleted\")\n",
    "\n",
    "#----------------------(3) DELETE S3 OBJECTS------------------------------\n",
    "\n",
    "def delete_s3_objects(bucket_name):\n",
    "\n",
    "    bucket = s3_resource.Bucket(bucket_name)\n",
    "\n",
    "    for item in bucket.objects.all():\n",
    "        item.delete()\n",
    "    print(\"S3 bucket objects deleted\")\n",
    "\n",
    "    s3_resource.Bucket(bucket_name).delete()\n",
    "    print(\"S3 bucket deleted\")\n",
    "\n",
    "\n",
    "#---------------------(4) DELETE DYNAMO DB TABLE--------------------------\n",
    "\n",
    "def delete_table(table_object):\n",
    "    table_object.delete()\n",
    "    print(\"Dynamo DB table deleted\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test SQS queue with launched cloud architecture by sending test survey submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "PATH = '/Users/mariagabrielaayala/Desktop/Spring 2023/Big Data and High Performance Computing/a6-magabrielaa/data/'\n",
    "\n",
    "lst_survey_paths = []\n",
    "\n",
    "files = os.listdir(PATH)\n",
    "\n",
    "for f in files:\n",
    "    lst_survey_paths.append(PATH + f)\n",
    "\n",
    "# Send survey responses to SQS queue with 10 second delay in between\n",
    "for survey_path in lst_survey_paths:\n",
    "    time.sleep(10)\n",
    "    survey_response = send_survey(survey_path, queue_url)\n",
    "    print(survey_response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print a list of objects in S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0001092821120000.json', '0001092921120000.json', '0001093021120300.json', '0002092821120000.json', '0003092821120001.json', '0004092821120002.json', '0005092821122000.json']\n"
     ]
    }
   ],
   "source": [
    "# Print list of objects in S3 bucket\n",
    "response = s3.list_objects(Bucket=\"mariagabrielaa-a6\")\n",
    "lst_objects = []\n",
    "for obj in response['Contents']:\n",
    "    lst_objects.append(obj['Key'])\n",
    "\n",
    "print(lst_objects)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query records in Dynamo DB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q1': Decimal('1'), 'q2': Decimal('1'), 'user_id': '0001', 'q3': Decimal('2'), 'q4': Decimal('2'), 'q5': Decimal('2'), 'num_submission': Decimal('3'), 'freetext': \"I lost my car keys this afternoon at lunch, so I'm more stressed than normal\"}\n",
      "{'q1': Decimal('4'), 'q2': Decimal('1'), 'user_id': '0002', 'q3': Decimal('1'), 'q4': Decimal('1'), 'q5': Decimal('3'), 'num_submission': Decimal('1'), 'freetext': \"I'm having a great day!\"}\n",
      "{'q1': Decimal('1'), 'q2': Decimal('3'), 'user_id': '0003', 'q3': Decimal('3'), 'q4': Decimal('1'), 'q5': Decimal('4'), 'num_submission': Decimal('1'), 'freetext': 'It was a beautiful, sunny day today.'}\n",
      "{'q1': Decimal('1'), 'q2': Decimal('1'), 'user_id': '0004', 'q3': Decimal('1'), 'q4': Decimal('1'), 'q5': Decimal('1'), 'num_submission': Decimal('1'), 'freetext': 'I had a very bad day today...'}\n",
      "{'q1': Decimal('3'), 'q2': Decimal('3'), 'user_id': '0005', 'q3': Decimal('3'), 'q4': Decimal('3'), 'q5': Decimal('3'), 'num_submission': Decimal('1'), 'freetext': \"I'm feeling okay, but not spectacular\"}\n"
     ]
    }
   ],
   "source": [
    "# Query Dynamo DB table\n",
    "lst_users = ['0001', '0002', '0003', '0004', '0005']\n",
    "\n",
    "for user in lst_users:\n",
    "        response = table.get_item(\n",
    "                Key={'user_id': user}\n",
    "                )\n",
    "        print(response['Item'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1(c) \n",
    "\n",
    "####  Your PI, who is overseeing this project, is worried that if all of the participants in the study (potentially thousands) submit surveys at the same time in the day, this might cause the system to crash and your lab might lose data (this happened to your PI when they ran a similar digital survey via on-premise servers in the early 2000s). How would you reassure your PI that your architecture is scalable and will be able to handle such spikes in demand? Your response should be at least 200 words and discuss the scalabilty of each of the cloud services you used in your pipeline in detail.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cloud architecture that sustains this pipeline is designed for scalability.\n",
    "\n",
    "First, when a user submits a survey, that submission is posted to an AWS **SQS queue**, which is durable and can process a near unlimited number of concurrent messages. SQS is designed as a distributive service that can scale when demand increases. AWS has many virtual machines distributed across availability zones and regions, so that when there is a spike in demand, messages are automatically distributed across servers. This allows SQS to handle a large volume of messages and process them in parallel.\n",
    "\n",
    "In our pipeline, we are using a standard queue instead of a “First In First Out” FIFO queue because it is not necessary to process messages in order or serially. This improves scalability because messages are processed in **parallel**, reducing processing times. Speed is relevant when there are spikes in demand, as that could be the source for a potential bottleneck. Also, standard queues support  **“at least once”** message delivery, which means that if a message could not be processed for any reason, it returns to the queue after the visibility timeout, to be re-processed by Lambda. This helps to prevent any loss of data and avoid the PI’s previous negative experience. Only when the message is susccessfully processed will it be deleted by Lambda.\n",
    "\n",
    "Second, the SQS queue triggers a **Lambda function**, which is **event-driven**, meaning that it only needs to be called when there is one or more messages to be processed. This helps with costs and scalability as the event-driven architecture ensures resources are not wasted. Lambda uses **event source mapping** to process messages from the SQS queue, where it **batches** the messages and invokes the mapped lambda function with a **concurrency** of up to 1000 invocations for a single Lambda function. Considering one invocation can process multiple messages, this is a very high concurrency number. Lambda scales out or in automatically on our behalf, depending on demand.\n",
    "\n",
    "In standard queues, like ours, the event source mapping polls the queue to process incoming messages beginning at 5 concurrent batches with 5 functions at a time. When there is a spike in demand, Lambda scales out adding “up to 60 functions per minute up to 1,000 functions” to process those messages. If the spike of demand is too high, one potential issue is that Lambda may reach the account’s **reserved concurrency** limit. In this case, the message will be returned to the queue for re-processing and depending on our queue settings, it will retry to process the message a certain number of times before going to the Dead Letter Queue. In any case, there are many options on how to handle messages in this rare situation without losing any survey submissions.\n",
    "\n",
    "Third, we use an** S3 data lake** to store **raw** survey data, which is also **distributed** across availability zones and regions. This makes the service scalable and fault-tolerant. Another benefit is that it can store **infinitely-sized** and amount of data, so it will have no issues with a spike in demand, in particular considering the survey entries are small data. S3 can write and read into storage in **parallel**, which makes it very fast and scalable. Essentially, we can have multiple invocations of Lambda writing data into S3 at the same time with no issues. In our case, we are only writing data into S3, but if we were to read it in for additional processing, S3 uses the object key as a hashing mechanism to partition the data and find it quickly, allowing for parallel reads as needed.\n",
    "\n",
    "Fourth, the pipeline ends by writing/updating one item per user id into a **Dynamo DB** table that essentially keeps track of the amount of survey submissions for each user.  As a **NoSQL** solution, Dynamo’s benefit is its **high throughput** that allows for many small concurrent reads and writes, which is exactly what we are doing with this study, in which we are making a lot of small updated to the table based on real-time survey submissions. Dynamo DB also has essentially **infinite storage** and it is highly flexible, reason why it scales out well. In our case, we are using Dynamo DB only to keep track of the amount of survey submissions or the latest submission per entry, so we only need to handle light SQL queries, which Dynamo can handle well.\n",
    "\n",
    "\n",
    "Reference: https://aws.amazon.com/blogs/compute/introducing-maximum-concurrency-of-aws-lambda-functions-when-using-amazon-sqs-as-an-event-source/\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tear down cloud architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda Function Deleted\n",
      "Event source mapping for 'a6' function deleted\n",
      "SQS Queue Deleted\n",
      "S3 bucket objects deleted\n",
      "S3 bucket deleted\n",
      "Dynamo DB table deleted\n"
     ]
    }
   ],
   "source": [
    "delete_lambda('a6')\n",
    "delete_queue(queue_url)\n",
    "delete_s3_objects('mariagabrielaa-a6')\n",
    "delete_table(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
